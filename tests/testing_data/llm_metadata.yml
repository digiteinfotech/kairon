openai:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "Open AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: "integer"
      default: 300
      minimum: 5
      maximum: 4096
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion using ChatGPT."
    model:
      type: "string"
      default: "gpt-4.1-mini"
      enum: ["gpt-3.5-turbo", "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1"]
      description: "The model hyperparameter is the ID of the model to use such as gpt-2, gpt-3, or a custom model that you have trained or fine-tuned."
    top_p:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    n:
      type: "integer"
      default: 1
      minimum: 1
      maximum: 5
      description: "The n hyperparameter controls the number of different response options that are generated by the model."
    stop:
      anyOf:
        - type: "string"
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "integer"
        - type: "null"
      type:
        - "string"
        - "array"
        - "integer"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
    presence_penalty:
      type: "number"
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The presence_penalty hyperparameter penalizes the model for generating words that are not present in the context or input prompt."
    frequency_penalty:
      type: "number"
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The frequency_penalty hyperparameter penalizes the model for generating words that have already been generated in the current response."
    logit_bias:
      type: "object"
      default: {}
      description: "The logit_bias hyperparameter helps prevent GPT-3 from generating unwanted tokens or even to encourage generation of tokens that you do want."
anthropic:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "Anthropic AI Models for Prompt"
  properties:
    max_tokens:
      type: "integer"
      default: 1024
      minimum: 5
      maximum: 4096
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion using ChatGPT."
    model:
      type: "string"
      default: "claude-3-7-sonnet-20250219"
      enum: ["claude-3-7-sonnet-20250219"]
      description: "The model hyperparameter is the ID of the Anthropic model or a custom model that you have trained or fine-tuned."
gemini:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "Google AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: "integer"
      default: 300
      minimum: 5
      maximum: 4096
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion."
    model:
      type: "string"
      default: "gemini/gemini-2.0-flash"
      enum: ["gemini/gemini-2.0-flash", "gemini/gemini-2.0-flash-exp"]
      description: "The model hyperparameter is the ID of the model to use."
    top_p:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    n:
      type: "integer"
      default: 1
      minimum: 1
      maximum: 5
      description: "The n hyperparameter controls the number of different response options that are generated by the model."
    stop:
      anyOf:
        - type: "string"
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "integer"
        - type: "null"
      type:
        - "string"
        - "array"
        - "integer"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
perplexity:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "Perplexity AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 0.2
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      anyOf:
        - type: "integer"
          minimum: 5
          maximum: 127072
        - type: "null"
      type:
        - "integer"
        - "null"
      default: null
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion"
    model:
      type: "string"
      default: "perplexity/llama-3.1-sonar-small-128k-online"
      enum: ["perplexity/llama-3.1-sonar-small-128k-online", "perplexity/llama-3.1-sonar-large-128k-online", "perplexity/llama-3.1-sonar-huge-128k-online"]
    search_domain_filter:
      anyOf:
        - type: "array"
          maxItems: 3
          items:
            type: "string"
        - type: "null"
      type:
        - "array"
        - "null"
      default: null
      description: "The search domain filter hyperparameter is used to specify list of domain to be used by online models."
    search_recency_filter:
      anyOf:
        - type: "string"
          enum: ["month", "week", "day", "hour"]
        - type: "null"
      type:
        - "string"
        - "null"
      default: null
      description: "return results from specified time interval"
    top_p:
      type: "number"
      default: 0.9
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    top_k:
      type: "integer"
      default: 0
      minimum: 0
      maximum: 2048
      description: "The top_k hyperparameter controls the number of token to keep for top_k filtering"
    presence_penalty:
      type: "number"
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The presence_penalty hyperparameter penalizes the model for generating words that are not present in the context or input prompt."
    frequency_penalty:
      type: "number"
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The frequency_penalty hyperparameter penalizes the model for generating words that have already been generated in the current response."
aws-nova:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "AWS Nova AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: "integer"
      default: 300
      minimum: 5
      maximum: 5120
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion."
    model:
      type: "string"
      default: "bedrock/converse/us.amazon.nova-lite-v1:0"
      enum: ["bedrock/converse/us.amazon.nova-micro-v1:0", "bedrock/converse/us.amazon.nova-lite-v1:0", "bedrock/converse/us.amazon.nova-pro-v1:0"]
      description: "The model hyperparameter is the ID of the model to use such as gpt-2, gpt-3, or a custom model that you have trained or fine-tuned."
    top_p:
      type: "number"
      default: 0.0
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    stop:
      anyOf:
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "null"
      type:
        - "array"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
aws-llama:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "AWS llama AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 1.0
      minimum: 0.0
      maximum: 1.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: "integer"
      default: 300
      minimum: 0
      maximum: 1000000
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion."
    model:
      type: "string"
      default: "bedrock/us.meta.llama4-maverick-17b-instruct-v1:0"
      enum: ["bedrock/us.meta.llama4-maverick-17b-instruct-v1:0", "bedrock/us.meta.llama4-scout-17b-instruct-v1:0"]
      description: "The model hyperparameter is the ID of the model to use such as gpt-2, gpt-3, or a custom model that you have trained or fine-tuned."
    top_k:
      type: "integer"
      default: 250
      minimum: 0
      maximum: 500
      description: "The top_k hyperparameter controls the number of token to keep for top_k filtering"
    top_p:
      type: "number"
      default: 0.999
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    stop:
      anyOf:
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "null"
      type:
        - "array"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."


openrouter:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: "object"
  description: "Openrouter AI Models for Prompt"
  properties:
    temperature:
      type: "number"
      default: 1.0
      minimum: 0.0
      maximum: 1.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: "integer"
      default: 300
      minimum: 5
      maximum: 5120
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion."
    model:
      type: "string"
      default: "gpt-4.1"
      enum:  ["gpt-3.5-turbo", "gpt-4.1-nano", "gpt-4.1-mini", "gpt-4.1", "anthropic/claude-3-7-sonnet-20250219", "google/gemini-2.5-flash-image", "google/gemini-2.0-flash-001","google/gemini-2.0-flash-exp" ]
      description: "The model hyperparameter is the ID of the model to use such as gpt-2, gpt-3, or a custom model that you have trained or fine-tuned."
    top_k:
      type: "integer"
      default: 250
      minimum: 0
      maximum: 500
      description: "The top_k hyperparameter controls the number of token to keep for top_k filtering"
    top_p:
      type: "number"
      default: 0.999
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    stop:
      anyOf:
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "null"
      type:
        - "array"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
