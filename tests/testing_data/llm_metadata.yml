openai:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: object
  description: "Open AI Models for Prompt"
  properties:
    temperature:
      type: number
      default: 0.0
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: integer
      default: 300
      minimum: 5
      maximum: 4096
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion using ChatGPT."
    model:
      type: string
      default: "gpt-4o-mini"
      enum: ["gpt-3.5-turbo", "gpt-3.5-turbo-instruct", "gpt-4o-mini"]
      description: "The model hyperparameter is the ID of the model to use such as gpt-2, gpt-3, or a custom model that you have trained or fine-tuned."
    top_p:
      type: number
      default: 0.0
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    n:
      type: integer
      default: 1
      minimum: 1
      maximum: 5
      description: "The n hyperparameter controls the number of different response options that are generated by the model."
    stop:
      anyOf:
        - type: "string"
        - type: "array"
          maxItems: 4
          items:
            type: "string"
        - type: "integer"
        - type: "null"
      type:
        - "string"
        - "array"
        - "integer"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
    presence_penalty:
      type: number
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The presence_penalty hyperparameter penalizes the model for generating words that are not present in the context or input prompt."
    frequency_penalty:
      type: number
      default: 0.0
      minimum: -2.0
      maximum: 2.0
      description: "The frequency_penalty hyperparameter penalizes the model for generating words that have already been generated in the current response."
    logit_bias:
      type: object
      default: {}
      description: "The logit_bias hyperparameter helps prevent GPT-3 from generating unwanted tokens or even to encourage generation of tokens that you do want."

claude:
  $schema: "https://json-schema.org/draft/2020-12/schema"
  type: object
  description: "Claude Models for Prompt"
  properties:
    temperature:
      type: number
      default: 0.5
      minimum: 0.0
      maximum: 2.0
      description: "The temperature hyperparameter controls the creativity or randomness of the generated responses."
    max_tokens:
      type: integer
      default: 200
      minimum: 10
      maximum: 3000
      description: "The max_tokens hyperparameter limits the length of generated responses in chat completion using Claude."
    model:
      type: string
      default: "claude-v1"
      enum: ["claude-v1", "claude-v2"]
      description: "The model hyperparameter is the ID of the Claude model to use."
    top_p:
      type: number
      default: 0.9
      minimum: 0.0
      maximum: 1.0
      description: "The top_p hyperparameter is a value that controls the diversity of the generated responses."
    n:
      type: integer
      default: 1
      minimum: 1
      maximum: 3
      description: "The n hyperparameter controls the number of different response options that are generated by the model."
    stop:
      anyOf:
        - type: "string"
        - type: "array"
          maxItems: 3
          items:
            type: "string"
        - type: "integer"
        - type: "null"
      type:
        - "string"
        - "array"
        - "integer"
        - "null"
      default: null
      description: "The stop hyperparameter is used to specify a list of tokens that should be used to indicate the end of a generated response."
    presence_penalty:
      type: number
      default: 0.1
      minimum: -1.0
      maximum: 1.0
      description: "The presence_penalty hyperparameter penalizes the model for generating words that are not present in the context or input prompt."
    frequency_penalty:
      type: number
      default: 0.1
      minimum: -1.0
      maximum: 1.0
      description: "The frequency_penalty hyperparameter penalizes the model for generating words that have already been generated in the current response."
    logit_bias:
      type: object
      default: {}
      description: "The logit_bias hyperparameter helps prevent Claude from generating unwanted tokens or even to encourage generation of tokens that you do want."
